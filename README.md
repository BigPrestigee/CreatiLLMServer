# vllm-demo

* Qwen-1_8B-Chat-Int4 | vllm | gradio | fastapi | nvicore
* 高并发http llm推理服务端学习和搭建

# pre

* GPU Memory > 6 GB
* python >= 3.8
* torch >= 2.1
* cuda == 2.1
* vllm-gptq for Qwen
* modelscope
* tiktoken

# how to run?

* python server.py
* python client.py or python gradio_ui.py

# test_demo
![Demo Image](https://github.com/BigPrestigee/CreatiLLMServer/blob/main/demo.jpg?raw=true)
